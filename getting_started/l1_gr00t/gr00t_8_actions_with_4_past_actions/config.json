{
  "_name_or_path": "/home/longnh/datpv/Isaac-GR00T/getting_started/l1_gr00t",
  "action_dim": 32,
  "action_head_cfg": {
    "action_backbone_cfg": {
      "attention_head_dim": 48,
      "dropout": 0.2,
      "final_dropout": true,
      "interleave_self_attention": true,
      "norm_type": "layer_norm",
      "num_attention_heads": 32,
      "num_layers": 16,
      "output_dim": 1024,
      "positional_embeddings": false
    },
    "action_dim": 32,
    "action_horizon": 8,
    "add_cross_attention": false,
    "add_pos_embed": true,
    "freeze_decode_layer": false,
    "hidden_size": 1024,
    "input_embedding_dim": 1536,
    "load_pretrained_det_decode_layer_path": null,
    "max_action_dim": 32,
    "max_num_embodiments": 32,
    "max_seq_len": 512,
    "max_state_dim": 64,
    "model_dtype": "float32",
    "tune_action_backbone": true,
    "tune_projector": true,
    "num_past_actions": 4
  },
  "action_horizon": 8,
  "architectures": [
    "GR00T_N1"
  ],
  "attn_implementation": null,
  "backbone_cfg": {
    "allow_reshape_visual": true,
    "load_pretrained_det_eagle_path": null,
    "model_name": "$GR00T_BACKBONE_PATH/eagle2_hg_model",
    "processor_cfg": {
      "max_input_tiles": 1,
      "model_path": "$GR00T_BACKBONE_PATH/eagle2_hg_model",
      "model_spec": {
        "num_image_token": 64,
        "template": "qwen2-chat"
      }
    },
    "projector_dim": 2048,
    "remove_llm": false,
    "reproject_vision": false,
    "scale_image_resolution": 1,
    "select_layer": 12,
    "tune_llm": false,
    "tune_visual": true
  },
  "compute_dtype": "bfloat16",
  "hidden_size": 1536,
  "model_dtype": "float32",
  "model_type": "gr00t_n1",
  "torch_dtype": "bfloat16",
  "transformers_version": "4.45.2"
}
